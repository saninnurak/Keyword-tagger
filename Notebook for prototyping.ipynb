{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fed1e0a-fb21-4d9b-993c-9ba2c2220199",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1 - prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36059aee-ab73-433b-9b5a-2d69bd4e4d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:11:25] Loading tags from 'data/tags.csv'...\n",
      "[10:11:25] Loaded 27 tags.\n",
      "[10:11:25] Loading sentences from 'data/sentences.txt'...\n",
      "[10:11:25] Loaded 2997 sentences.\n",
      "[10:11:25] Tagging sentences...\n",
      "[10:11:25] Processed 100 sentences...\n",
      "[10:11:25] Processed 200 sentences...\n",
      "[10:11:25] Processed 300 sentences...\n",
      "[10:11:25] Processed 400 sentences...\n",
      "[10:11:25] Processed 500 sentences...\n",
      "[10:11:25] Processed 600 sentences...\n",
      "[10:11:25] Processed 700 sentences...\n",
      "[10:11:25] Processed 800 sentences...\n",
      "[10:11:25] Processed 900 sentences...\n",
      "[10:11:25] Processed 1000 sentences...\n",
      "[10:11:25] Processed 1100 sentences...\n",
      "[10:11:25] Processed 1200 sentences...\n",
      "[10:11:25] Processed 1300 sentences...\n",
      "[10:11:25] Processed 1400 sentences...\n",
      "[10:11:25] Processed 1500 sentences...\n",
      "[10:11:25] Processed 1600 sentences...\n",
      "[10:11:25] Processed 1700 sentences...\n",
      "[10:11:25] Processed 1800 sentences...\n",
      "[10:11:25] Processed 1900 sentences...\n",
      "[10:11:25] Processed 2000 sentences...\n",
      "[10:11:25] Processed 2100 sentences...\n",
      "[10:11:25] Processed 2200 sentences...\n",
      "[10:11:25] Processed 2300 sentences...\n",
      "[10:11:25] Processed 2400 sentences...\n",
      "[10:11:25] Processed 2500 sentences...\n",
      "[10:11:25] Processed 2600 sentences...\n",
      "[10:11:25] Processed 2700 sentences...\n",
      "[10:11:25] Processed 2800 sentences...\n",
      "[10:11:25] Processed 2900 sentences...\n",
      "[10:11:25] Tagging completed. Total: 2997. Time: 0.20 seconds.\n",
      "[10:11:25] Saving results to 'output/task_1_output.tsv'...\n",
      "[10:11:25] Sentences with tags: 1892\n",
      "[10:11:25] Sentences without tags: 1105\n",
      "[10:11:25] Total sentences processed: 2997\n",
      "[10:11:25] Results saved successfully.\n",
      "[{'sentence': 'Get a loan', 'tags': ''}, {'sentence': 'testing', 'tags': ''}, {'sentence': 'Pay off car', 'tags': 'Personal Loans, Vehicle Loan'}, {'sentence': 'Hi there! I am in the process of switching banks because my Husband and I are joining accounts. We are both switching to a join account through SoFi. How should I go about transferring my current checking and savings account?', 'tags': 'Vehicle Loan, Commercial Loans, Accounts'}, {'sentence': 'bank phone number', 'tags': ''}, {'sentence': 'Hi there! I need some help with‚Ä¶login', 'tags': ''}, {'sentence': 'i put a application\\xa0 in for a checking account on your website and was wondering the staus on that', 'tags': 'Accounts'}, {'sentence': 'Hi, I just had a question on what would be the best course of action for a large withdraw or payment. I am getting a new car in a few weeks and plan on making a large down payment. Would something like that be best to write a cashier‚Äôs check or can I simply have them take it out of checking? I‚Äôll of course have to move the amount from savings to checking first. Thanks!', 'tags': 'Vehicle Loan, Tax Information'}, {'sentence': 'Hi there! I need some help with‚Ä¶ my debit card I forgot the pin', 'tags': 'Vehicle Loan, Debit Cards, Retail Banking'}, {'sentence': 'I‚Äôm not seeing my account', 'tags': ''}]\n",
      "[10:11:25] Finished entire process in 0.22 seconds.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper to print with timestamp\n",
    "def log(message):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "# Load tags and keywords\n",
    "def load_tags(path):\n",
    "    log(f\"Loading tags from '{path}'...\")\n",
    "    tags = []\n",
    "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                keyword_list = ast.literal_eval(row['keywords'])\n",
    "                tags.append({\n",
    "                    'id': row['id'],\n",
    "                    'name': row['name'],\n",
    "                    'keywords': set(kw.lower() for kw in keyword_list)  # set for faster lookup\n",
    "                })\n",
    "            except Exception as e:\n",
    "                log(f\"Error parsing row {row['id']}: {e}\")\n",
    "    log(f\"Loaded {len(tags)} tags.\")\n",
    "    return tags\n",
    "\n",
    "# Load sentences from file\n",
    "def load_sentences(path):\n",
    "    log(f\"Loading sentences from '{path}'...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        sentences = [line.strip() for line in f if line.strip()]\n",
    "    log(f\"Loaded {len(sentences)} sentences.\")\n",
    "    return sentences\n",
    "\n",
    "# Match tags to sentences\n",
    "def tag_sentences(sentences, tags):\n",
    "    log(\"Tagging sentences...\")\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    for i, sentence in enumerate(sentences, start=1):\n",
    "        sentence_lower = sentence.lower()\n",
    "        matched = [\n",
    "            tag['name'] for tag in tags\n",
    "            if any(keyword in sentence_lower for keyword in tag['keywords'])\n",
    "        ]\n",
    "        results.append({'sentence': sentence, 'tags': ', '.join(matched)})\n",
    "        if i % 100 == 0:\n",
    "            log(f\"Processed {i} sentences...\")\n",
    "    log(f\"Tagging completed. Total: {len(results)}. Time: {time.time() - start_time:.2f} seconds.\")\n",
    "    return results\n",
    "\n",
    "# Save to TSV\n",
    "def save_results(results, output_path):\n",
    "    log(f\"Saving results to '{output_path}'...\")\n",
    "    with_tag = 0\n",
    "    without_tag = 0\n",
    "    \n",
    "    for result in results:\n",
    "        if result['tags']:  # if tags string is not empty\n",
    "            with_tag += 1\n",
    "        else:\n",
    "            without_tag += 1\n",
    "    \n",
    "    log(f\"Sentences with tags: {with_tag}\")\n",
    "    log(f\"Sentences without tags: {without_tag}\")\n",
    "    log(f\"Total sentences processed: {len(results)}\")\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['sentence', 'tags'], delimiter='\\t')\n",
    "        writer.writerows(results)\n",
    "    log(\"Results saved successfully.\")\n",
    "\n",
    "# Main pipeline\n",
    "def main():\n",
    "    start = time.time()\n",
    "    tags = load_tags('data/tags.csv')\n",
    "    sentences = load_sentences('data/sentences.txt')\n",
    "    results = tag_sentences(sentences, tags)\n",
    "    save_results(results, 'output/task_1_output.tsv')\n",
    "    print(results[:10])\n",
    "    log(f\"Finished entire process in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd1d1b-4c75-4ee0-be1b-dfdeaea7b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 - Prototyping - Not satisfied with solution !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd0777b-f783-4484-8afb-3e963462a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings using model: paraphrase-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409dcd88e33f49a89b8a602de43695eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings using model: paraphrase-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0a4045c39049148c16f39054e8168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training classifier...\n",
      "üîç Predicting tags with threshold: 0.5\n",
      "\n",
      "üîç Preview of first 10 predictions:\n",
      "Get a loan\t\n",
      "testing\t\n",
      "Pay off car\t\n",
      "Hi there! I am in the process of switching banks because my Husband and I are joining accounts. We are both switching to a join account through SoFi. How should I go about transferring my current checking and savings account?\t\n",
      "bank phone number\t\n",
      "Hi there! I need some help with‚Ä¶login\t\n",
      "i put a application¬† in for a checking account on your website and was wondering the staus on that\t\n",
      "Hi, I just had a question on what would be the best course of action for a large withdraw or payment. I am getting a new car in a few weeks and plan on making a large down payment. Would something like that be best to write a cashier‚Äôs check or can I simply have them take it out of checking? I‚Äôll of course have to move the amount from savings to checking first. Thanks!\t\n",
      "Hi there! I need some help with‚Ä¶ my debit card I forgot the pin\t\n",
      "I‚Äôm not seeing my account\t\n",
      "\n",
      "‚úÖ Total sentences with predictions: 94\n",
      "üíæ Predictions saved to: output/task_2_output.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "def load_data(tags_path: str, sentences_path: str) -> (pd.DataFrame, List[str]):\n",
    "    \"\"\"Loads tag data and test sentences from file.\"\"\"\n",
    "    try:\n",
    "        task_df = pd.read_csv(tags_path)\n",
    "        task_df['keywords'] = task_df['keywords'].apply(eval)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error loading tags: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(sentences_path, 'r', encoding='utf-8') as f:\n",
    "            sentences = [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Error loading sentences: {e}\")\n",
    "\n",
    "    return task_df, sentences\n",
    "\n",
    "def build_training_data(task_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Builds training dataframe from keywords and tags.\"\"\"\n",
    "    pairs = [(kw.lower(), row['name']) for _, row in task_df.iterrows() for kw in row['keywords']]\n",
    "    train_df = pd.DataFrame(pairs, columns=['sentence', 'tag'])\n",
    "    return train_df.groupby('sentence')['tag'].apply(list).reset_index()\n",
    "\n",
    "def encode_labels(tags: pd.Series) -> (MultiLabelBinarizer, np.ndarray):\n",
    "    \"\"\"Encodes multilabel tags.\"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(tags)\n",
    "    return mlb, y\n",
    "\n",
    "def generate_embeddings(model_name: str, sentences: List[str]) -> np.ndarray:\n",
    "    \"\"\"Generates sentence embeddings using a transformer model.\"\"\"\n",
    "    print(f\"üîÑ Generating embeddings using model: {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(sentences, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def train_model(X: np.ndarray, y: np.ndarray) -> OneVsRestClassifier:\n",
    "    \"\"\"Trains a One-vs-Rest Random Forest classifier.\"\"\"\n",
    "    print(\"üß† Training classifier...\")\n",
    "    clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def predict_tags(clf, X_test: np.ndarray, mlb: MultiLabelBinarizer, threshold: float) -> List[List[str]]:\n",
    "    \"\"\"Predicts tags for test data using probability thresholding.\"\"\"\n",
    "    print(f\"üîç Predicting tags with threshold: {threshold}\")\n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    return mlb.inverse_transform(y_pred)\n",
    "\n",
    "def save_predictions(sentences: List[str], predictions: List[List[str]], output_path: str):\n",
    "    \"\"\"Saves predictions to a TSV file.\"\"\"\n",
    "    output_lines = []\n",
    "    predicted_count = 0\n",
    "\n",
    "    for sentence, tags in zip(sentences, predictions):\n",
    "        line = f\"{sentence}\\t{', '.join(tags)}\" if tags else f\"{sentence}\\t\"\n",
    "        output_lines.append(line)\n",
    "        if tags:\n",
    "            predicted_count += 1\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "\n",
    "    print(\"\\nüîç Preview of first 10 predictions:\")\n",
    "    for line in output_lines[:10]:\n",
    "        print(line)\n",
    "\n",
    "    print(f\"\\n‚úÖ Total sentences with predictions: {predicted_count}\")\n",
    "    print(f\"üíæ Predictions saved to: {output_path}\")\n",
    "\n",
    "def main(threshold: float = 0.2):\n",
    "    tags_path = 'data/tags.csv'\n",
    "    sentences_path = 'data/sentences.txt'\n",
    "    output_path = 'output/task_2_output.tsv'\n",
    "    model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "    task_df, sentences = load_data(tags_path, sentences_path)\n",
    "    grouped_train = build_training_data(task_df)\n",
    "    \n",
    "    mlb, y_train = encode_labels(grouped_train['tag'])\n",
    "    X_train = generate_embeddings(model_name, grouped_train['sentence'].tolist())\n",
    "    X_test = generate_embeddings(model_name, sentences)\n",
    "    \n",
    "    clf = train_model(X_train, y_train)\n",
    "    predicted_tags = predict_tags(clf, X_test, mlb, threshold)\n",
    "    \n",
    "    save_predictions(sentences, predicted_tags, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(threshold=0.5)  # Adjust threshold here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f095889-f662-4c69-bc8d-a7024144d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2. Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df9edbef-0ecb-44d4-a30f-35996bcf1617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings with: paraphrase-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062999b216714d16aaa821bee200cdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating embeddings with: paraphrase-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e9015c91fa4894b2feb199ad8f19e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training neural network...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0312 - loss: 0.7012 - val_accuracy: 0.0492 - val_loss: 0.5952\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0314 - loss: 0.5686 - val_accuracy: 0.0492 - val_loss: 0.4751\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0299 - loss: 0.4453 - val_accuracy: 0.0492 - val_loss: 0.3565\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0451 - loss: 0.3286 - val_accuracy: 0.0492 - val_loss: 0.2594\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0454 - loss: 0.2464 - val_accuracy: 0.0656 - val_loss: 0.2029\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0798 - loss: 0.1959 - val_accuracy: 0.0656 - val_loss: 0.1784\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1538 - loss: 0.1703 - val_accuracy: 0.0656 - val_loss: 0.1684\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2359 - loss: 0.1601 - val_accuracy: 0.0984 - val_loss: 0.1630\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2449 - loss: 0.1506 - val_accuracy: 0.1148 - val_loss: 0.1590\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2890 - loss: 0.1449 - val_accuracy: 0.1311 - val_loss: 0.1555\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3126 - loss: 0.1366 - val_accuracy: 0.1967 - val_loss: 0.1527\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3329 - loss: 0.1341 - val_accuracy: 0.2131 - val_loss: 0.1493\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3596 - loss: 0.1306 - val_accuracy: 0.2459 - val_loss: 0.1457\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3726 - loss: 0.1293 - val_accuracy: 0.2787 - val_loss: 0.1425\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4268 - loss: 0.1200 - val_accuracy: 0.3279 - val_loss: 0.1398\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4498 - loss: 0.1169 - val_accuracy: 0.3770 - val_loss: 0.1368\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4777 - loss: 0.1142 - val_accuracy: 0.3934 - val_loss: 0.1339\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4993 - loss: 0.1074 - val_accuracy: 0.3934 - val_loss: 0.1310\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5432 - loss: 0.1024 - val_accuracy: 0.3934 - val_loss: 0.1285\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5419 - loss: 0.1020 - val_accuracy: 0.4262 - val_loss: 0.1256\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5186 - loss: 0.0990 - val_accuracy: 0.4262 - val_loss: 0.1240\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5978 - loss: 0.0977 - val_accuracy: 0.4754 - val_loss: 0.1215\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5371 - loss: 0.0958 - val_accuracy: 0.4754 - val_loss: 0.1195\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5868 - loss: 0.0929 - val_accuracy: 0.4754 - val_loss: 0.1179\n",
      "Epoch 25/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6589 - loss: 0.0843 - val_accuracy: 0.4754 - val_loss: 0.1158\n",
      "Epoch 26/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6303 - loss: 0.0820 - val_accuracy: 0.4754 - val_loss: 0.1143\n",
      "Epoch 27/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5990 - loss: 0.0862 - val_accuracy: 0.4590 - val_loss: 0.1126\n",
      "Epoch 28/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6663 - loss: 0.0793 - val_accuracy: 0.4754 - val_loss: 0.1114\n",
      "Epoch 29/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6465 - loss: 0.0792 - val_accuracy: 0.4754 - val_loss: 0.1103\n",
      "Epoch 30/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7058 - loss: 0.0745 - val_accuracy: 0.4754 - val_loss: 0.1088\n",
      "Epoch 31/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7024 - loss: 0.0709 - val_accuracy: 0.4754 - val_loss: 0.1078\n",
      "Epoch 32/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7265 - loss: 0.0723 - val_accuracy: 0.4918 - val_loss: 0.1061\n",
      "Epoch 33/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6948 - loss: 0.0710 - val_accuracy: 0.4918 - val_loss: 0.1052\n",
      "Epoch 34/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7512 - loss: 0.0658 - val_accuracy: 0.4918 - val_loss: 0.1045\n",
      "Epoch 35/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7319 - loss: 0.0665 - val_accuracy: 0.5082 - val_loss: 0.1037\n",
      "Epoch 36/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7855 - loss: 0.0621 - val_accuracy: 0.4754 - val_loss: 0.1033\n",
      "Epoch 37/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7624 - loss: 0.0625 - val_accuracy: 0.4918 - val_loss: 0.1024\n",
      "Epoch 38/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7828 - loss: 0.0608 - val_accuracy: 0.4918 - val_loss: 0.1016\n",
      "Epoch 39/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7817 - loss: 0.0593 - val_accuracy: 0.4918 - val_loss: 0.1008\n",
      "Epoch 40/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8023 - loss: 0.0575 - val_accuracy: 0.4754 - val_loss: 0.1006\n",
      "Epoch 41/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7508 - loss: 0.0588 - val_accuracy: 0.4754 - val_loss: 0.0995\n",
      "Epoch 42/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8233 - loss: 0.0554 - val_accuracy: 0.4918 - val_loss: 0.0985\n",
      "Epoch 43/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8132 - loss: 0.0540 - val_accuracy: 0.4754 - val_loss: 0.0974\n",
      "Epoch 44/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8098 - loss: 0.0524 - val_accuracy: 0.4918 - val_loss: 0.0975\n",
      "Epoch 45/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8107 - loss: 0.0486 - val_accuracy: 0.4918 - val_loss: 0.0967\n",
      "Epoch 46/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8053 - loss: 0.0530 - val_accuracy: 0.4918 - val_loss: 0.0965\n",
      "Epoch 47/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8061 - loss: 0.0512 - val_accuracy: 0.4918 - val_loss: 0.0958\n",
      "Epoch 48/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8145 - loss: 0.0510 - val_accuracy: 0.4918 - val_loss: 0.0958\n",
      "Epoch 49/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8482 - loss: 0.0469 - val_accuracy: 0.4918 - val_loss: 0.0956\n",
      "Epoch 50/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8067 - loss: 0.0489 - val_accuracy: 0.4918 - val_loss: 0.0948\n",
      "üîç Predicting with threshold 0.3...\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "üîç Preview of first 10 predictions:\n",
      "Get a loan\t\n",
      "testing\tRetail Banking\n",
      "Pay off car\tVehicle Loan\n",
      "Hi there! I am in the process of switching banks because my Husband and I are joining accounts. We are both switching to a join account through SoFi. How should I go about transferring my current checking and savings account?\tAccounts, Marital Status\n",
      "bank phone number\t\n",
      "Hi there! I need some help with‚Ä¶login\tOnline Accounts\n",
      "i put a application¬† in for a checking account on your website and was wondering the staus on that\t\n",
      "Hi, I just had a question on what would be the best course of action for a large withdraw or payment. I am getting a new car in a few weeks and plan on making a large down payment. Would something like that be best to write a cashier‚Äôs check or can I simply have them take it out of checking? I‚Äôll of course have to move the amount from savings to checking first. Thanks!\tRetail Banking\n",
      "Hi there! I need some help with‚Ä¶ my debit card I forgot the pin\tCredit Card\n",
      "I‚Äôm not seeing my account\t\n",
      "\n",
      "‚úÖ Total sentences with predictions: 1781\n",
      "üíæ Predictions saved to: output/task_2_output.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def load_data(tags_path: str, sentences_path: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    task_df = pd.read_csv(tags_path)\n",
    "    task_df['keywords'] = task_df['keywords'].apply(eval)\n",
    "    with open(sentences_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = [line.strip() for line in f if line.strip()]\n",
    "    return task_df, sentences\n",
    "\n",
    "def build_training_data(task_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pairs = [(kw.lower(), row['name']) for _, row in task_df.iterrows() for kw in row['keywords']]\n",
    "    train_df = pd.DataFrame(pairs, columns=['sentence', 'tag'])\n",
    "    return train_df.groupby('sentence')['tag'].apply(list).reset_index()\n",
    "\n",
    "def encode_labels(tags: pd.Series) -> Tuple[MultiLabelBinarizer, np.ndarray]:\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(tags)\n",
    "    return mlb, y\n",
    "\n",
    "def generate_embeddings(model_name: str, sentences: List[str]) -> np.ndarray:\n",
    "    print(f\"üîÑ Generating embeddings with: {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(sentences, show_progress_bar=True)\n",
    "\n",
    "def build_nn_model(input_dim: int, output_dim: int) -> Sequential:\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='sigmoid')  # sigmoid for multilabel\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model_nn(X: np.ndarray, y: np.ndarray) -> Sequential:\n",
    "    model = build_nn_model(X.shape[1], y.shape[1])\n",
    "    print(\"üß† Training neural network...\")\n",
    "    model.fit(X, y, epochs=50, batch_size=32, validation_split=0.1,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=5)], verbose=1)\n",
    "    return model\n",
    "\n",
    "def predict_tags_nn(model, X_test: np.ndarray, mlb: MultiLabelBinarizer, threshold: float) -> List[List[str]]:\n",
    "    print(f\"üîç Predicting with threshold {threshold}...\")\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    return mlb.inverse_transform(y_pred)\n",
    "\n",
    "def save_predictions(sentences: List[str], predictions: List[List[str]], output_path: str):\n",
    "    output_lines = []\n",
    "    predicted_count = 0\n",
    "    for sentence, tags in zip(sentences, predictions):\n",
    "        line = f\"{sentence}\\t{', '.join(tags)}\" if tags else f\"{sentence}\\t\"\n",
    "        output_lines.append(line)\n",
    "        if tags:\n",
    "            predicted_count += 1\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(output_lines))\n",
    "    print(\"\\nüîç Preview of first 10 predictions:\")\n",
    "    for line in output_lines[:10]:\n",
    "        print(line)\n",
    "    print(f\"\\n‚úÖ Total sentences with predictions: {predicted_count}\")\n",
    "    print(f\"üíæ Predictions saved to: {output_path}\")\n",
    "\n",
    "def main(threshold: float = 0.2):\n",
    "    tags_path = 'data/tags.csv'\n",
    "    sentences_path = 'data/sentences.txt'\n",
    "    output_path = 'output/task_2_output.tsv'\n",
    "    model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "    task_df, sentences = load_data(tags_path, sentences_path)\n",
    "    grouped_train = build_training_data(task_df)\n",
    "\n",
    "    mlb, y_train = encode_labels(grouped_train['tag'])\n",
    "    X_train = generate_embeddings(model_name, grouped_train['sentence'].tolist())\n",
    "    X_test = generate_embeddings(model_name, sentences)\n",
    "\n",
    "    model = train_model_nn(X_train, y_train)\n",
    "    predicted_tags = predict_tags_nn(model, X_test, mlb, threshold)\n",
    "\n",
    "    save_predictions(sentences, predicted_tags, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(threshold=0.3)  # Lower threshold for broader predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9558e-90f4-4bdd-9572-12081b58e01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
